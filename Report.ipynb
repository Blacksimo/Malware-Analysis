{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Malware Analysis\n",
    "\n",
    "Faricelli Simone 1647406\n",
    "\n",
    "## Introduction\n",
    "The purpose of this project is to develop a valuable tool to recognize and classify different kind of Malware Application, running on AndroidOS, through the use of several Machine Learning Classifiers.\n",
    "\n",
    "In order to train the above-mentioned classifiers, we're taking advantage of the [Drebin Dataset](https://www.sec.cs.tu-bs.de/~danarp/drebin/index.html), which provide us 5,560 malevolent files from 179 different malware families, collected in the period of August 2010 to October 2012.\n",
    "\n",
    "From those applications we are extracting features through the manifest.xml file and the disassembled code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Classification Problem\n",
    "Classification belongs to the category of supervised learning where the targets (or labels or categories) are also provided with the input data and it is, in fact, the process of predicting the class of given data points. Classification predictive modeling is the task of approximating a target function (f) from input variables (X) to discrete output variables (y).\n",
    "\n",
    "The input variable chosen for this project is a list of vectors, each of which specify, for each malware, the features of the application itself, previously retrieved from a dictionary of all the features.\n",
    "\n",
    "As expected, the output variable is a list containing the category of each malware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "### Selective Extraction\n",
    "The Drebin Dataset mentioned above contains not only examples of features of malware files, but also a huge quantity of non-malware applications, which are not useful for our classification goal.\n",
    "\n",
    "A selective extraction is therefore performed through a python script, picking only the malware files, specified in the csv file, from the 130.000 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting  5560  files from the Drebin Dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████| 5560/5560 [01:58<00:00, 47.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "csvName = 'sha256_family.csv'\n",
    "malware = list()\n",
    "with open('dataset/' + csvName) as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        malware.append(row[0])\n",
    "\n",
    "print(\"Extracting \",len(malware),\" files from the Drebin Dataset\")\n",
    "\n",
    "with ZipFile('drebin.zip') as myzip:\n",
    "    for i in tqdm(range(len(malware))):\n",
    "        myzip.extract('drebin/' + csvName)\n",
    "        myzip.extract('drebin/feature_vectors/' + malware[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development\n",
    "### Libraries and Global Variables\n",
    "\n",
    "In the following section the required libraries are imported and the global variables initialized. For this project, wide use of the python library for machine learning called \"scikit-learn\" is done. That particular library needs to be installed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import os, sys, csv, time, json\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "datasetFolder = 'drebin/'\n",
    "featuresFolder = 'feature_vectors/'\n",
    "csvName = 'sha256_family.csv'\n",
    "dictionaryName = 'dictionary.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Malware Dataset & Dictionary\n",
    "The provided dataset is composed of examples of malware and non-malware data, but in order to train the classifier the focus is moved onto the malevolent files. In the block below, those two categories are being separated using the cvs file as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of malwares in the dataset:\t 5560\n"
     ]
    }
   ],
   "source": [
    "# List of all the files in debrin dataset\n",
    "dataset = os.listdir(datasetFolder + featuresFolder)\n",
    "\n",
    "# Define list for malware and non-malware files\n",
    "malware = list()\n",
    "\n",
    "# Create malware dictionary from the csv file, with the file name and the type of each one\n",
    "with open(datasetFolder + csvName) as csvFile:\n",
    "    reader = csv.reader(csvFile)\n",
    "    next(reader)\n",
    "    malwareDictionary = {row[0]: row[1] for row in reader}\n",
    "\n",
    "# Separate the file names\n",
    "for i in dataset:\n",
    "    if i in malwareDictionary:\n",
    "        malware.append(i)\n",
    "\n",
    "print('Number of malwares in the dataset:\\t', len(malware))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature and Malware Categories\n",
    "For a matter of knowledge, the features contained in the dataset and written into the files, are being collected, listed and printed. Also empty files are removed from the feature analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty file:  3de513a148400b457dd8d8fa9238804db3ec031a0b526d4a04b77e5112aa2dcf\n",
      "Empty file:  6c6eeed1b91913db0d6232edb1979c67d6fb48ca3da4f83dc49fb565a4e5f4fe\n",
      "Empty file:  76e91e1f9cc3422c333e51b65bb98dd50d00f1f45a15d2008807b06c125e651a\n",
      "Empty file:  df2c357f513c270cd1d06418e4eaf64aeb6b2d947149e83ed4f42c88286b76a7\n",
      "Empty file:  f6239ba0487ffcf4d09255dba781440d2600d3c509e66018e6a5724912df34a9\n",
      "The features can be divided in 10 different sets:\n",
      "\t api_call\n",
      "\t feature\n",
      "\t url\n",
      "\t service_receiver\n",
      "\t permission\n",
      "\t call\n",
      "\t intent\n",
      "\t real_permission\n",
      "\t activity\n",
      "\t provider\n"
     ]
    }
   ],
   "source": [
    "# Collect all the features from the files of the dataset\n",
    "categoryList = list()\n",
    "for file in malware:\n",
    "    with open(datasetFolder + featuresFolder + file) as f:\n",
    "        content = f.readlines()\n",
    "        for line in content:\n",
    "            try:\n",
    "                category, string = line.split('::')\n",
    "                if category not in categoryList:\n",
    "                    categoryList.append(category)\n",
    "            except:\n",
    "                print(\"Empty file: \", file)\n",
    "\n",
    "print('The features can be divided in',\n",
    "      len(categoryList), 'different sets:')\n",
    "print('\\t', '\\n\\t '.join(categoryList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the features, also malware categories are being collected from the malware dictionary created above, and taken into account only if the number of their samples is greater that 20. From this list, the output vector for the training will be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 339   GinMaster\n",
      "\t 613   Opfake\n",
      "\t 925   FakeInstaller\n",
      "\t  27   Boxer\n",
      "\t  29   Jifake\n",
      "\t 667   DroidKungFu\n",
      "\t  41   SMSreg\n",
      "\t  58   Gappusin\n",
      "\t 147   Kmin\n",
      "\t  61   FakeRun\n",
      "\t 625   Plankton\n",
      "\t  28   Hamob\n",
      "\t 132   FakeDoc\n",
      "\t  91   Adrd\n",
      "\t  37   Yzhc\n",
      "\t  69   Glodream\n",
      "\t 330   BaseBridge\n",
      "\t 152   Iconosys\n",
      "\t  70   ExploitLinuxLotoor\n",
      "\t  59   SendPay\n",
      "\t  92   Geinimi\n",
      "\t  81   DroidDream\n",
      "\t  43   Imlog\n",
      "\t  69   MobileTx\n",
      "\t 775   Other\n"
     ]
    }
   ],
   "source": [
    "# Collect all the classes from the files of the dataset\n",
    "classes = []\n",
    "counter = []\n",
    "for key, value in sorted(malwareDictionary.items()):\n",
    "    if not value in classes:\n",
    "        classes.append(value)\n",
    "        counter.append(1)\n",
    "    else:\n",
    "        counter[classes.index(value)] += 1\n",
    "\n",
    "# Considering only classes with more than 20 samples\n",
    "classes.append(\"Other\")\n",
    "counter.append(0)\n",
    "classesNew = list()\n",
    "counterNew = list()\n",
    "\n",
    "for i, el in enumerate(counter):\n",
    "    if el > 20:\n",
    "        classesNew.append(classes[i])\n",
    "        counterNew.append(el)\n",
    "    else:\n",
    "        counter[len(counter)-1] += counter[i]\n",
    "\n",
    "classes = classesNew\n",
    "counter = counterNew\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    print('\\t', '{:>3}'.format(counter[i]), \" \", classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chosen Ones\n",
    "In order to reduce the input vector dimension and fine-tune the training, only a few features are chosen to be extracted. Deeper consideration will be done later about this specific selection.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosenFeatures = [\"permission\", \"api_call\", \"service_receiver\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Extraction\n",
    "The following function has the purpose to, given as input the feature name and its subsequent string, identify the best function to extract the feature and return as output a vector of words which will be used to build the dictionary of words or the vector of features.\n",
    "\n",
    "The algorithm presented here is using just the most important word of each line of the document, for example touchscreen and MAIN, and discarding words less relevant (android, hardware, intent and action) as the lines shown below.\n",
    "\n",
    "```\n",
    "feature::android.hardware.touchscreen\n",
    "intent::android.intent.action.MAIN\n",
    "```\n",
    "\n",
    "The alghoritm perform that way also because the firsts are common words which do not improve the training results, because they add no more information to the input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selfExtraction(feature, string):\n",
    "    def extractUrl(string):\n",
    "        try:\n",
    "            baseUrl = \"{0.scheme}://{0.netloc}/\".format(urlsplit(string))\n",
    "            if len(baseUrl) > 10:\n",
    "                return [baseUrl]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractApiCall(string):\n",
    "        try:\n",
    "            string = string.replace(';->', '/')\n",
    "            apiCall = string.split('/')\n",
    "            return apiCall\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractFeature(string):\n",
    "        try:\n",
    "            feature = string.split('.')[-1]\n",
    "            return [feature]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractPermission(string):\n",
    "        try:\n",
    "            permission = string.split('.')[-1].lower()\n",
    "            return [permission]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractCall(string):\n",
    "        try:\n",
    "            call = string.lower()\n",
    "            return [call]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractActivity(string):\n",
    "        try:\n",
    "            activity = string.split('.')[-1].lower()\n",
    "            if activity[0] == \":\":\n",
    "                activity = activity[1:]\n",
    "            return [activity]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractIntent(string):\n",
    "        try:\n",
    "            intent = string.split('.')[-1].lower()\n",
    "            return [intent]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractServiceReceiver(string):\n",
    "        try:\n",
    "            serviceReceiver = string.split('.')[-1].lower()\n",
    "            return [serviceReceiver]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def extractProvider(string):\n",
    "        try:\n",
    "            provider = string.split('.')[-1].lower()\n",
    "            if \"\\\\\" in provider:\n",
    "                provider = provider[:-2]\n",
    "            return [provider]\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    switcher = {\n",
    "        'url': extractUrl,\n",
    "        'api_call': extractApiCall,\n",
    "        'feature': extractFeature,\n",
    "        'permission': extractPermission,\n",
    "        'real_permission': extractPermission,\n",
    "        'call': extractCall,\n",
    "        'activity': extractActivity,\n",
    "        'intent': extractIntent,\n",
    "        'service_receiver': extractServiceReceiver,\n",
    "        'provider': extractProvider\n",
    "    }\n",
    "    return switcher[feature](string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words Indices Vector\n",
    "The network is fed with a set of vectors, which elements are the indices of each word belonging to the specific file, mapped from a dictionary of all the words obtained from the feature extraction of the entire dataset of files. \n",
    "\n",
    "The dictionary is previously generated through the same function, as shown below. \n",
    "\n",
    "For instance, considering the file\n",
    "```\n",
    "612aa197794bc4c88bd7dbed41a9b13b9969befb645761fa384b1a567a50ceee\n",
    "```\n",
    "Its own Word Vectore will be\n",
    "```\n",
    "[0, 25, 26, 126, 111, 22, 3, 7, 8, 10, 11, 33, 19]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processFile(file, dictionary, outputCreation = False, dictionaryCreation=False):\n",
    "    # List of words of each file\n",
    "    words = list()\n",
    "    # Read line by line of the file\n",
    "    with open(datasetFolder + featuresFolder + file) as f:\n",
    "        content = f.readlines()\n",
    "\n",
    "        # Divide each line of document in feature name and its subsequent string\n",
    "        for line in content:\n",
    "            try:\n",
    "                split = line.split('::')\n",
    "                category = split[0]\n",
    "                string = split[1]\n",
    "            except:\n",
    "                break\n",
    "            \n",
    "            wordList = None\n",
    "            if category in chosenFeatures:\n",
    "                wordList = selfExtraction(category, string)\n",
    "\n",
    "            # If able to extract feature from line\n",
    "            if wordList != None:\n",
    "                for word in wordList:\n",
    "                    word = word.replace('\\n', '')\n",
    "                    # If flagged to create the dictionary\n",
    "                    if dictionaryCreation:\n",
    "                        if not word in dictionary:\n",
    "                            index = len(dictionary)\n",
    "                            dictionary[word] = index\n",
    "                    else:\n",
    "                        index = dictionary[word]\n",
    "                        if index not in words:\n",
    "                            words.append(index)\n",
    "        #If flagged to create the output vector of categories\n",
    "        if outputCreation:\n",
    "            if malwareDictionary[file] in classes:\n",
    "                y.append(classes.index(malwareDictionary[file]))\n",
    "            else:\n",
    "                y.append(classes.index(\"Other\"))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary Generation and Loading\n",
    "The above-mentioned dictionary is, at first, searched into the main folder and, if not found, generated and stored in a json file for a better readablity and due to its huge size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary file not found!\n",
      "Creating dictionary\n",
      "\n",
      "Dictionary saved to file!\n",
      "Dictionary size:  1733\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(dictionaryName):\n",
    "    print('Dictionary file found!')\n",
    "    with open(dictionaryName) as d:\n",
    "        dictionary = json.load(d)\n",
    "    d.close()\n",
    "\n",
    "else:\n",
    "    print('Dictionary file not found!')\n",
    "    \n",
    "    # Define the dictionary\n",
    "    dictionary = {}\n",
    "    \n",
    "    # Colect words for malware\n",
    "    print('Creating dictionary')\n",
    "    for i in range(len(dataset)):\n",
    "        processFile(dataset[i], dictionary, dictionaryCreation=True)\n",
    "    \n",
    "    with open(dictionaryName, 'w+') as d:\n",
    "        d.write(json.dumps(dictionary, sort_keys=True, indent=4))\n",
    "    d.close()\n",
    "        \n",
    "\n",
    "    print('\\nDictionary saved to file!')\n",
    "\n",
    "# Print number of words in the dictionary\n",
    "dictionarySize = len(dictionary)\n",
    "print('Dictionary size: ', dictionarySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Input and Output\n",
    "Through the above-mentioned function used to extract features from the malware files, the actual input (X) and output (y) that will feed the network are finally built. \n",
    "\n",
    "In order to have a homogeneus size of input, the one-hot encoded version of the word vectors are built, starting from the length of the dictionary to generate a list containing only zeros, and replacing that values with ones at the indices specified by the output of the feature extraction function. One vector for file is then appended to the input list.\n",
    "\n",
    "Moreover, for each file, the category index of that malware is appended to the output list, setting to True the boolean variable \"outputCreation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features\n",
      "Extraction Complete\n"
     ]
    }
   ],
   "source": [
    "#List of features\n",
    "X = list()\n",
    "#List of categories\n",
    "y = list()\n",
    "# Construct the vector of features and output vector at the same time\n",
    "def featuresExtraction(file, dictionary):\n",
    "    indices = processFile(file, dictionary, outputCreation=True)\n",
    "    feat = [0] * len(dictionary)\n",
    "    for i in indices:\n",
    "        feat[i-1] = 1\n",
    "\n",
    "    return feat\n",
    "# Extract features and append to the list of features\n",
    "print('Extracting Features')\n",
    "for i in range(len(malware)):\n",
    "    feat = featuresExtraction(malware[i], dictionary)\n",
    "    X.append(feat)\n",
    "print('Extraction Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split\n",
    "In order to have a good evaluation of the used algorithm, it is important to separate the dataset between training and test set. The training set will be used to train the classifier and the test set will only be used to calculate the metrics.\n",
    "\n",
    "It is important for the classifier do not see the test set before, because it could overfit the data and not be able to generalize for new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split randomly the dataset into train and evaluation \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers\n",
    "For this project, three classifiers were chosen to be trained with the same dataset. The first classifier is the Naive Bayes, the second one is Support Vector Machines and the third one is Random Forest.\n",
    "\n",
    "Follows a brief description of the above-mentioned classifiers, highlighting the pros and cons for each method.\n",
    "\n",
    "### Naive Bayes \n",
    "Naive Bayes is a probabilistic classifier inspired by the Bayes theorem under a simple assumption which is the attributes are conditionally independent.\n",
    "\n",
    "The classification is conducted by deriving the maximum posterior which is the maximal P(Ci|X) with the above assumption applying to Bayes theorem. This assumption greatly reduces the computational cost by only counting the class distribution. Even though the assumption is not valid in most cases since the attributes are dependent, surprisingly Naive Bayes has able to perform impressively.\n",
    "\n",
    "Naive Bayes is a very simple algorithm to implement and good results have obtained in most cases. It can be easily scalable to larger datasets since it takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\n",
    "\n",
    "Naive Bayes can suffer from a problem called the zero probability problem. When the conditional probability is zero for a particular attribute, it fails to give a valid prediction. This needs to be fixed explicitly using a Laplacian estimator.\n",
    "\n",
    "### Support Vector Machine\n",
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes.\n",
    "\n",
    "SVM is mostly useful in non-linear separation problems, because it has a technique called the kernel trick, these are functions which takes low dimensional input space and transform it to a higher dimensional space, which means that it converts not separable problem to separable problem.\n",
    "\n",
    "SVM performs similarly to logistic regression (really well) when with clear margins of linear separation.\n",
    "\n",
    "On the contrary, it does not perform well when we have large data set, because the required training time is higher, and when the dataset has more noise, which means that target classes are overlapping.\n",
    "\n",
    "Tuning Parameters?\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "Random Forest is an example of an ensemble method, in which we combine multiple machine learning algorithms to obtain better predictive performance. We’ll run multiple models on the data and use the aggregate predictions, which will be better than a single model alone.\n",
    "\n",
    "The idea behind a Random Forest is actually pretty simple: We repeatedly select data from the data set (with replacement) and build a Decision Tree with each new sample. It is important to note that since we are sampling with replacement, many data points will be repeated and many won’t be included as well. This is important to keep in mind when we talk about measuring error of a Random Forest. Another important feature of the Random Forest is that each node of the Decision Tree is limited to only considering splits on random subsets of the features.\n",
    "\n",
    "One big advantage of Random Forest is, that it can be used for both classification and regression problems, which form the majority of current machine learning systems.\n",
    "\n",
    "On the contrary, Random Forest produces results not easy to visually interpret and it can overfit with some datasets with noisy classification/regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Results\n",
    "\n",
    "### Training Function\n",
    "\n",
    "After having all the functions for the analysis explained and implemented, it is possible to train it with the training data and validate with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "MultinomialNB\n",
      "****Results****\n",
      "Accuracy: 85.8813%\n",
      "Log Loss: 1.1335253415771336\n",
      "Time to Train: 0.6 s\n",
      "==============================\n",
      "SVC\n",
      "****Results****\n",
      "Accuracy: 95.7734%\n",
      "Log Loss: 0.20580616263825374\n",
      "Time to Train: 48.0 s\n",
      "==============================\n",
      "RandomForestClassifier\n",
      "****Results****\n",
      "Accuracy: 95.6835%\n",
      "Log Loss: 0.27335448783949584\n",
      "Time to Train: 2.97 s\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "y_predict = list()\n",
    "classifiers = [\n",
    "        MultinomialNB(),\n",
    "        SVC(kernel=\"linear\", probability=True),\n",
    "        RandomForestClassifier(n_estimators=100, max_depth=100)\n",
    "]\n",
    "log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\", \"Time\"]\n",
    "log = pd.DataFrame(columns=log_cols)\n",
    "\n",
    "for clf in classifiers:\n",
    "    t=time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    name = clf.__class__.__name__\n",
    "    \n",
    "    print(\"=\"*30)\n",
    "    print(name)\n",
    "    \n",
    "    print('****Results****')\n",
    "    train_predictions = clf.predict(X_test)\n",
    "    y_predict.append(train_predictions)\n",
    "    acc = accuracy_score(y_test, train_predictions)\n",
    "    print(\"Accuracy: {:.4%}\".format(acc))\n",
    "    \n",
    "    train_predictions = clf.predict_proba(X_test)\n",
    "    ll = log_loss(y_test, train_predictions)\n",
    "    print(\"Log Loss: {}\".format(ll))\n",
    "\n",
    "    timeSpent = round(t2-t, 2)\n",
    "    print(\"Time to Train: {} s\".format(timeSpent))\n",
    "    \n",
    "    log_entry = pd.DataFrame([[name, acc*100, ll, timeSpent]], columns=log_cols)\n",
    "    log = log.append(log_entry)\n",
    "    \n",
    "print(\"=\"*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")\n",
    "\n",
    "plt.xlabel('Accuracy %')\n",
    "plt.title('Classifier Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/accuracy.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")\n",
    "\n",
    "plt.xlabel('Log Loss')\n",
    "plt.title('Classifier Log Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/logloss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_color_codes(\"muted\")\n",
    "sns.barplot(x='Time', y='Classifier', data=log, color=\"r\")\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.title('Classifier Time to Train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/timetotrain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Considerations\n",
    "\n",
    "### Choosen Features and Classifiers\n",
    "A wide range of attempts has been made in order to find the categories that performs better with this classification problem.\n",
    "Almost all the possibile combinations between the features have been tried, highlighting that, category such as \"feature\" or \"provider\", which generates alone a dictionary containing less than 50 words, do not increase the accuracy when combined with others category, therefore they'have been deleted from the list.\n",
    "\n",
    "On the other side, there are categories such as \"permission\" or \"api_call\" that perform amazing results all alone (an average of 90% of accuracy with the three classifiers). It has been easy to maximize the perfomance starting from those categories.\n",
    "\n",
    "In all of the attempts, Random Forest and SVM performed a lot better than Naive Bayes, so the final goal of the task had been maximizing the perfomance of that specific classifiers. That combination has been found in the categories \"permission\", \"service_receiver\" and \"activity\".\n",
    "\n",
    "### From the Graph\n",
    "\n",
    "The graphs showed above highlight streghts and weaknesses of each classifier, which is an extension of what it has been already told. \n",
    "\n",
    "Besides the fact that Naive Bayes performs worse than the other two in most of the cases, also the time spent to train the classifier needs to be considered. It's clear that the Support Vector Machine is the slowest one and that's because its fit time complexity is more than quadratic with the number of samples, which makes it hard to scale to huge dataset (in this case, samples number equal to the number of malware files).\n",
    "\n",
    "From the results of this project it's cristal clear that, among the studied three, the classifier that best fit this classification problem is the Random Forest.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
